{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, List, Tuple, Dict, Any, Iterable, TypeVar, Type, NamedTuple, Sequence, Generic, _GenericAlias, get_origin, get_args\n",
    "from typing_extensions import Self\n",
    "from abc import abstractmethod\n",
    "\n",
    "from beartype import beartype\n",
    "from edf_interface.data.base import DataAbstractBase, _device, _dtype, _torch_tensor_to\n",
    "from edf_interface.data.se3 import SE3\n",
    "from edf_interface.data.pointcloud import PointCloud\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [SE3(poses=torch.tensor([[1., 0., 0., 0., 0., 0., 1.]])).to('cuda'),\n",
    "          SE3(poses=torch.tensor([[1., 0., 0., 0., 0., 0., 1.]])).to('cuda'),\n",
    "          SE3(poses=torch.tensor([[1., 0., 0., 0., 0., 0., 1.]])).to('cuda'),\n",
    "          SE3(poses=torch.tensor([[1., 0., 0., 0., 0., 0., 1.]])).to('cuda'),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "class DataList(DataAbstractBase):\n",
    "    metadata_args: List[str] = ['name']\n",
    "    data_seq: List[DataAbstractBase]\n",
    "    _data_name_prefix: str = 'data_'\n",
    "    \n",
    "    @property\n",
    "    def data_args_type(self) -> Dict[str, type]:\n",
    "        outputs = {}\n",
    "        for i, data in enumerate(self.data_seq):\n",
    "            outputs[f\"{self._data_name_prefix}{i}\"] = type(data)\n",
    "        return outputs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_seq)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Union[Self, DataAbstractBase]:\n",
    "        assert type(idx) == slice or type(idx) == int, \"Indexing must be an integer or a slice with single axis.\"\n",
    "        if type(idx) == int:\n",
    "            return self.data_seq[idx]\n",
    "        else:\n",
    "            return self.new(data_seq=self.data_seq[idx])\n",
    "        \n",
    "    @classmethod\n",
    "    def get_data_idx(cls, name: str) -> Optional[int]:\n",
    "        if name.startswith(cls._data_name_prefix):\n",
    "            index = name.lstrip(cls._data_name_prefix)\n",
    "            try:\n",
    "                index = int(name.lstrip(cls._data_name_prefix))\n",
    "                return index\n",
    "            except ValueError:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def __getattr__(self, name: str):\n",
    "        data_idx: Optional[int] = self.get_data_idx(name=name)\n",
    "        if data_idx is None:\n",
    "            # if hasattr(super(), '__getattr__'):\n",
    "            #     return super().__getattr__(name=name)\n",
    "            # else:\n",
    "            #     raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n",
    "            pass\n",
    "        else:\n",
    "            return self[data_idx]\n",
    "        \n",
    "    def __setattr__(self, name: str, value: Any):\n",
    "        data_idx: Optional[int] = self.get_data_idx(name=name)\n",
    "        if data_idx is None:\n",
    "            super().__setattr__(name, value)\n",
    "        else:\n",
    "            assert isinstance(value, DataAbstractBase), f\"{name} must be an isntance of DataAbstractBase\"\n",
    "            if data_idx == len(self):\n",
    "                self.data_seq.append(value)\n",
    "            elif data_idx < len(self):\n",
    "                self.data_seq[data_idx] = value\n",
    "            else:\n",
    "                raise IndexError(f\"{name} index larger than maximum lenghth: {len(self)}\")\n",
    "\n",
    "    def new(self, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Returns a new object which is a shallow copy of original object, but with data and metadata that are specified as kwargs being replaced. \n",
    "        \"\"\"\n",
    "        for arg in (['data_seq'] + list(self.metadata_args)):\n",
    "            if arg not in kwargs.keys():\n",
    "                kwargs[arg] = getattr(self, arg)\n",
    "\n",
    "        return self.__class__(**kwargs)\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self) -> bool:\n",
    "        if len(self) == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __init__(self, data_seq: Sequence[DataAbstractBase]):\n",
    "        self.data_seq = data_seq\n",
    "        super().__init__()\n",
    "        \n",
    "    @classmethod\n",
    "    def empty(cls, *args, **kwargs) -> Self:\n",
    "        return cls([], *args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> Optional[torch.device]:\n",
    "        if self.is_empty:\n",
    "            return None\n",
    "        \n",
    "        device = None\n",
    "        for data in self.data_seq:\n",
    "            if hasattr(data, 'device'):\n",
    "                device = data.device\n",
    "        \n",
    "        return device\n",
    "    \n",
    "    def to(self, *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        similar to pytorch Tensor objects' .to() method\n",
    "        \"\"\"\n",
    "        if self.is_empty:\n",
    "            return self\n",
    "        else:\n",
    "            data_seq = []\n",
    "            for data in self.data_seq:\n",
    "                if isinstance(data, DataAbstractBase):\n",
    "                    data = data.to(*args, **kwargs)\n",
    "                    data_seq.append(data)\n",
    "                elif isinstance(data, torch.Tensor):\n",
    "                    assert '__tensor' not in kwargs.keys(), f\"Don't use __tensor as a keyward arguments. It is reserved.\"\n",
    "                    data = _torch_tensor_to(data, *args, **kwargs)\n",
    "                    data_seq.append(data)\n",
    "                elif hasattr(data, 'to'):\n",
    "                    raise NotImplementedError(f\"'to()' is not implemented for data type {type(data)}\")\n",
    "                else:\n",
    "                    data_seq.append(data)\n",
    "\n",
    "            return self.new(data_seq=data_seq)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_data_dict(cls, data_dict: Dict[str, Any], *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Reconstruct data object from dictionary.\n",
    "        \"\"\"\n",
    "        inputs: Dict[str, Any] = {}\n",
    "        data_seq = []\n",
    "        for arg, val in data_dict.items():\n",
    "            if arg == 'metadata':\n",
    "                assert isinstance(val, Dict), f\"data_dict['metadata'] must be a dictionary but {type(val)} is provided.\"\n",
    "                assert cls.__name__ == val['__type__'], f\"Class type {cls.__name__} does not match with type annotated in metadata ({val['__type__']})\"\n",
    "            else:\n",
    "                data_idx = cls.get_data_idx(arg)\n",
    "                assert data_idx is not None, f\"Variable name must be like {cls._data_name_prefix}_(idx)\"\n",
    "                assert data_idx == len(data_seq)\n",
    "                from edf_interface.data import registered_datatype\n",
    "                assert hasattr(registered_datatype, val['__type__']), f\"Unknown data type {val['__type__']}\"\n",
    "                type_ = getattr(registered_datatype, val['__type__'])\n",
    "                assert issubclass(type_, DataAbstractBase), f\"{type_}\"\n",
    "                assert isinstance(val, Dict), f\"For arg of type {type(arg)}, data_dict[arg] must be a dictionary\"\n",
    "                assert 'metadata' in val.keys(), f\"For arg of type {type(arg)}, data_dict[arg] must be a dictionary, and has 'metadata' as a key\"\n",
    "                assert type_.__name__ == val['metadata']['__type__'], f\"{type_.__name__} != {val['metadata']['__type__']}\"\n",
    "                val = type_.from_data_dict(data_dict=val, *args, **kwargs)\n",
    "                data_seq.append(val)\n",
    "        inputs['data_seq'] = data_seq\n",
    "\n",
    "        if 'metadata' in data_dict.keys():\n",
    "            metadata = data_dict['metadata']\n",
    "            assert isinstance(metadata, Dict)\n",
    "            for arg in metadata.keys():\n",
    "                assert arg not in inputs.keys(), f\"metadata_arg {arg} already exists as a data argument!\"\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        input_kwargs = {}\n",
    "        for k,v in {**inputs, **metadata}.items():\n",
    "            if k=='__type__':  # __type__ is not required as an argument to the class constructor\n",
    "                pass\n",
    "            else:\n",
    "                input_kwargs[k] = v\n",
    "        return cls(**input_kwargs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataList(data_seq=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__type__': 'DataList', 'name': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_0': edf_interface.data.se3.SE3,\n",
       " 'data_1': edf_interface.data.se3.SE3,\n",
       " 'data_2': edf_interface.data.se3.SE3,\n",
       " 'data_3': edf_interface.data.se3.SE3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.data_args_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DataList>  (device: cuda:0)\n",
      "  Metadata: \n",
      "    - name: None\n",
      "  Data: \n",
      "    - data_0: <SE3>\n",
      "          - name: \n",
      "          - poses: <Tensor> (Shape: torch.Size([1, 7]))\n",
      "            \n",
      "          \n",
      "    - data_1: <SE3>\n",
      "          - name: \n",
      "          - poses: <Tensor> (Shape: torch.Size([1, 7]))\n",
      "            \n",
      "          \n",
      "    - data_2: <SE3>\n",
      "          - name: \n",
      "          - poses: <Tensor> (Shape: torch.Size([1, 7]))\n",
      "            \n",
      "          \n",
      "    - data_3: <SE3>\n",
      "          - name: \n",
      "          - poses: <Tensor> (Shape: torch.Size([1, 7]))\n",
      "            \n",
      "          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SE3>  (device: cuda:0)\n",
       "  Metadata: \n",
       "    - name: \n",
       "  Data: \n",
       "    - poses: <Tensor> (Shape: torch.Size([1, 7]))\n",
       "          tensor([[1., 0., 0., 0., 0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdfa\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfa' is not defined"
     ]
    }
   ],
   "source": [
    "sdfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edf_interface.data import registered_datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(registered_datatype, 'SE3').__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(registered_datatype, 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @beartype\n",
    "# class DataSequenceAbstract(DataAbstractBase):\n",
    "#     sequence_type: type = list\n",
    "#     data_seq: Sequence[DataAbstractBase]\n",
    "\n",
    "#     data_args_type: Dict[str, type] = {}\n",
    "#     metadata_args: List[str]\n",
    "    \n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.data_seq)\n",
    "    \n",
    "#     @property\n",
    "#     def is_empty(self) -> bool:\n",
    "#         if len(self) == 0:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "        \n",
    "#     @classmethod\n",
    "#     def empty(cls, *args, **kwargs) -> Self:\n",
    "#         return cls(cls.sequence_type(), *args, **kwargs)\n",
    "\n",
    "#     @property\n",
    "#     def device(self) -> torch.device:\n",
    "#         if self.is_empty:\n",
    "#             raise AttributeError(\"The 'device' property is ambiguous for empty data seqeunce.\")\n",
    "        \n",
    "#         return self.data_seq[0].device\n",
    "\n",
    "#     def __init__(self):\n",
    "#         assert not self.data_args_type, f\"Don't use self.data_args_hint.\"\n",
    "#         super().__init__()\n",
    "#         # for data in data_seq:\n",
    "#         #     assert type(data_type_list)\n",
    "#         # if device is None:\n",
    "#         #     self.data_seq = self.sequence_type(data for data in data_seq)\n",
    "#         # else:\n",
    "#         #     self.data_seq = self.sequence_type(data.to(device) for data in data_seq)\n",
    "\n",
    "#     def new(self, **kwargs) -> Self:\n",
    "#         \"\"\"\n",
    "#         Returns a new object which is a shallow copy of original object, but with data and metadata that are specified as kwargs being replaced. \n",
    "#         \"\"\"\n",
    "#         for arg in (['data_seq'] + list(self.metadata_args)):\n",
    "#             if arg not in kwargs.keys():\n",
    "#                 kwargs[arg] = getattr(self, arg)\n",
    "\n",
    "#         return self.__class__(**kwargs)\n",
    "    \n",
    "#     def to(self, *args, **kwargs) -> Self:\n",
    "#         if self.is_empty:\n",
    "#             return self\n",
    "#         else:\n",
    "#             return self.__class__(data_seq=self.sequence_type(data.to(*args, **kwargs) for data in self.data_seq))\n",
    "    \n",
    "#     def get_data_dict(self, *args, **kwargs) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Returns recursive data dictionary.\n",
    "#         Similar to torch.nn.Module's .state_dict() method.\n",
    "#         \"\"\"\n",
    "#         data_dict = {}\n",
    "#         for i, data in enumerate(self.data_seq):\n",
    "#             if isinstance(data, DataAbstractBase):\n",
    "#                 data = data.get_data_dict(*args, **kwargs)\n",
    "#             elif isinstance(data, torch.Tensor):\n",
    "#                 assert '__tensor' not in kwargs.keys(), f\"Don't use __tensor as a keyward arguments. It's reserved.\"\n",
    "#                 data = _torch_tensor_to(data, *args, **kwargs)\n",
    "#             data_dict[data] = data\n",
    "#         data_dict['metadata'] = self.metadata\n",
    "        \n",
    "#         return data_dict\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_data_dict(cls, data_dict: Dict[str, Any], *args, **kwargs) -> Self:\n",
    "#         inputs: Dict[str, Any] = {}\n",
    "#         for arg, val in data_dict.items():\n",
    "#             if arg == 'metadata':\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 assert arg in cls.data_args_hint.keys(), f\"Unknown data argument: {arg}\"\n",
    "#                 hint = cls.data_args_hint[arg]\n",
    "#                 if issubclass(hint, DataAbstractBase):\n",
    "#                     val = hint.from_data_dict(data_dict=val, *args, **kwargs)\n",
    "#                 else:\n",
    "#                     assert isinstance(val, hint), f\"type({arg}) = {type(val)} != {hint}\"\n",
    "#                     if isinstance(val, torch.Tensor):\n",
    "#                         val = _torch_tensor_to(__tensor=val, *args, **kwargs)\n",
    "#                 inputs[arg] = val\n",
    "        \n",
    "#         if 'metadata' in data_dict.keys():\n",
    "#             metadata = data_dict['metadata']\n",
    "#             assert isinstance(metadata, Dict)\n",
    "#             for arg, val in metadata.items():\n",
    "#                 assert arg not in inputs.keys(), f\"metadata_arg {arg} already exists as a data argument!\"\n",
    "#         else:\n",
    "#             metadata = {}\n",
    "        \n",
    "#         inputs = {**inputs, **metadata}\n",
    "        \n",
    "#         return cls(**inputs)\n",
    "    \n",
    "#     def __repr__(self, abbrv: bool = False) -> str:\n",
    "#         if abbrv:\n",
    "#             prefix = ''\n",
    "#             bullet = '- '\n",
    "#         else:\n",
    "#             prefix = '  '\n",
    "#             bullet = prefix + '  - '            \n",
    "\n",
    "#         if abbrv:\n",
    "#             repr = \"\"\n",
    "#         else:\n",
    "#             repr = f\"<{self.__class__.__name__}>  (device: {str(self.device)})\\n\"\n",
    "\n",
    "#         if not abbrv:\n",
    "#             repr += prefix + \"Metadata: \\n\"\n",
    "#         for arg in self.metadata_args_hint.keys():\n",
    "#             obj = getattr(self, arg)\n",
    "#             repr += bullet + f\"{arg}: {obj.__repr__()}\\n\"\n",
    "\n",
    "#         if not abbrv:\n",
    "#             repr += prefix + \"Data: \\n\"\n",
    "#         for arg in self.data_args_hint.keys():\n",
    "#             obj = getattr(self, arg)\n",
    "\n",
    "#             repr += bullet + f\"{arg}: <{type(obj).__name__}>\"\n",
    "#             if hasattr(obj, 'shape'):\n",
    "#                 repr += ' (Shape: ' + obj.shape.__repr__() + ')\\n'\n",
    "#                 if abbrv:\n",
    "#                     subrepr = ''\n",
    "#                 else:\n",
    "#                     subrepr: str = obj.__repr__()\n",
    "#             elif isinstance(obj, DataAbstractBase):\n",
    "#                 repr += '\\n'\n",
    "#                 if abbrv:\n",
    "#                     subrepr = ''\n",
    "#                 else:\n",
    "#                     subrepr: str = obj.__repr__(abbrv=True)\n",
    "#             else:\n",
    "#                 repr += '\\n'\n",
    "#                 if abbrv:\n",
    "#                     subrepr = ''\n",
    "#                 else:\n",
    "#                     subrepr: str = obj.__repr__()\n",
    "            \n",
    "#             if abbrv:\n",
    "#                 indent = ' ' * (len(bullet))\n",
    "#             else:\n",
    "#                 indent = ' ' * (len(bullet) + 4)\n",
    "#             subrepr = subrepr.replace('\\n', '\\n' + indent)\n",
    "#             subrepr += '\\n'\n",
    "#             repr += indent + subrepr\n",
    "\n",
    "#         return repr\n",
    "    \n",
    "#     def __str__(self) -> str:\n",
    "#         return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "class DataSequenceAbstract(DataAbstractBase):\n",
    "    data_list: List[DataAbstractBase]\n",
    "    data_type_list: List[type]\n",
    "    metadata_args_hint: Dict[str, type]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self) -> bool:\n",
    "        if len(self) == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    @classmethod\n",
    "    def empty(cls, *args, **kwargs) -> Self:\n",
    "        return cls([], *args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        if self.is_empty:\n",
    "            raise AttributeError(\"The 'device' property is ambiguous for empty DataList.\")\n",
    "        \n",
    "        return self.data_list[0].device\n",
    "\n",
    "    def __init__(self, data_list: Iterable[DataAbstractBase], device: Optional[Union[str, torch.device]] = None):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            self.data_list = list(data for data in data_list)\n",
    "        else:\n",
    "            self.data_list = list(data.to(device) for data in data_list)\n",
    "\n",
    "    def new(self, **kwargs) -> Self:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _torch_tensor_to(self, device: Optional[_device]=None, \n",
    "                         dtype: Optional[_dtype]=None, \n",
    "                         non_blocking: bool = False, \n",
    "                         copy: bool = False, \n",
    "                         *args, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _data_to(self, *args, **kwargs) -> Dict[str, DataAbstractBase]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def to(self, *args, **kwargs) -> Self:\n",
    "        if self.is_empty:\n",
    "            return self\n",
    "        else:\n",
    "            return self.__class__(data_list=list(data.to(*args, **kwargs) for data in self.data_list))\n",
    "    \n",
    "    def get_data_dict(self, *args, **kwargs) -> Dict[str, Any]:\n",
    "        data_dict = {}\n",
    "        for arg in self.data_args_hint.keys():\n",
    "            obj = getattr(self, arg)\n",
    "            if isinstance(obj, DataAbstractBase):\n",
    "                obj = obj.get_data_dict(*args, **kwargs)\n",
    "            elif isinstance(obj, torch.Tensor):\n",
    "                assert '__tensor' not in kwargs.keys(), f\"Don't use __tensor as a keyward arguments. It is reserved.\"\n",
    "                obj = _torch_tensor_to(__tensor = obj, *args, **kwargs)\n",
    "            data_dict[arg] = obj\n",
    "        data_dict['metadata'] = self.metadata\n",
    "        \n",
    "        return data_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def from_data_dict(cls, data_dict: Dict[str, Any], *args, **kwargs) -> Self:\n",
    "        inputs: Dict[str, Any] = {}\n",
    "        for arg, val in data_dict.items():\n",
    "            if arg == 'metadata':\n",
    "                continue\n",
    "            else:\n",
    "                assert arg in cls.data_args_hint.keys(), f\"Unknown data argument: {arg}\"\n",
    "                hint = cls.data_args_hint[arg]\n",
    "                if issubclass(hint, DataAbstractBase):\n",
    "                    val = hint.from_data_dict(data_dict=val, *args, **kwargs)\n",
    "                else:\n",
    "                    assert isinstance(val, hint), f\"type({arg}) = {type(val)} != {hint}\"\n",
    "                    if isinstance(val, torch.Tensor):\n",
    "                        val = _torch_tensor_to(__tensor=val, *args, **kwargs)\n",
    "                inputs[arg] = val\n",
    "        \n",
    "        if 'metadata' in data_dict.keys():\n",
    "            metadata = data_dict['metadata']\n",
    "            assert isinstance(metadata, Dict)\n",
    "            for arg, val in metadata.items():\n",
    "                assert arg not in inputs.keys(), f\"metadata_arg {arg} already exists as a data argument!\"\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        inputs = {**inputs, **metadata}\n",
    "        \n",
    "        return cls(**inputs)\n",
    "    \n",
    "    def __repr__(self, abbrv: bool = False) -> str:\n",
    "        if abbrv:\n",
    "            prefix = ''\n",
    "            bullet = '- '\n",
    "        else:\n",
    "            prefix = '  '\n",
    "            bullet = prefix + '  - '            \n",
    "\n",
    "        if abbrv:\n",
    "            repr = \"\"\n",
    "        else:\n",
    "            repr = f\"<{self.__class__.__name__}>  (device: {str(self.device)})\\n\"\n",
    "\n",
    "        if not abbrv:\n",
    "            repr += prefix + \"Metadata: \\n\"\n",
    "        for arg in self.metadata_args_hint.keys():\n",
    "            obj = getattr(self, arg)\n",
    "            repr += bullet + f\"{arg}: {obj.__repr__()}\\n\"\n",
    "\n",
    "        if not abbrv:\n",
    "            repr += prefix + \"Data: \\n\"\n",
    "        for arg in self.data_args_hint.keys():\n",
    "            obj = getattr(self, arg)\n",
    "\n",
    "            repr += bullet + f\"{arg}: <{type(obj).__name__}>\"\n",
    "            if hasattr(obj, 'shape'):\n",
    "                repr += ' (Shape: ' + obj.shape.__repr__() + ')\\n'\n",
    "                if abbrv:\n",
    "                    subrepr = ''\n",
    "                else:\n",
    "                    subrepr: str = obj.__repr__()\n",
    "            elif isinstance(obj, DataAbstractBase):\n",
    "                repr += '\\n'\n",
    "                if abbrv:\n",
    "                    subrepr = ''\n",
    "                else:\n",
    "                    subrepr: str = obj.__repr__(abbrv=True)\n",
    "            else:\n",
    "                repr += '\\n'\n",
    "                if abbrv:\n",
    "                    subrepr = ''\n",
    "                else:\n",
    "                    subrepr: str = obj.__repr__()\n",
    "            \n",
    "            if abbrv:\n",
    "                indent = ' ' * (len(bullet))\n",
    "            else:\n",
    "                indent = ' ' * (len(bullet) + 4)\n",
    "            subrepr = subrepr.replace('\\n', '\\n' + indent)\n",
    "            subrepr += '\\n'\n",
    "            repr += indent + subrepr\n",
    "\n",
    "        return repr\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
