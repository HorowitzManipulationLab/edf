{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from typing import Tuple, Dict, Optional, Union, Any\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from edf.pc_utils import draw_geometry, create_o3d_points, get_plotly_fig\n",
    "from edf.data import PointCloud, SE3, TargetPoseDemo, DemoSequence, DemoSeqDataset, gzip_load\n",
    "from edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from edf.agent import PickAgent, PlaceAgent\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision= 3, sci_mode=False, linewidth=120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define utility functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_pointcloud(**kwargs) -> Tuple[PointCloud, PointCloud]:\n",
    "\n",
    "    ################### Write your custom codes here ###################\n",
    "    dir, idx, pick_or_place = kwargs['dir'], kwargs['idx'], kwargs['pick_or_place']\n",
    "\n",
    "    demos = DemoSeqDataset(dataset_dir=\"demo/test_demo\", annotation_file=\"data.yaml\")\n",
    "    demo: DemoSequence = demos[idx]\n",
    "    if pick_or_place == 'pick':\n",
    "        demo: TargetPoseDemo = demo[0]\n",
    "    elif pick_or_place == 'place':\n",
    "        demo: TargetPoseDemo = demo[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong value for pick_or_place argument: {pick_or_place}\")\n",
    "\n",
    "    scene_pcd: PointCloud = demo.scene_pc\n",
    "    grasp_pcd: PointCloud = demo.grasp_pc\n",
    "    target_pose: SE3 = demo.target_poses\n",
    "    ####################################################################\n",
    "\n",
    "    return scene_pcd, grasp_pcd\n",
    "\n",
    "\n",
    "def visualize(scene_pcd: PointCloud, grasp_pcd: PointCloud, pose: SE3, sampled_poses: Optional[SE3] = None, query: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "    \n",
    "    grasp_pl = grasp_pcd.plotly(point_size=1.0, name=\"grasp\")\n",
    "    grasp_geometry = [grasp_pl]\n",
    "    if query is not None:\n",
    "        query_points, query_attention = query\n",
    "        query_opacity = query_attention ** 1\n",
    "        query_pl = PointCloud.points_to_plotly(pcd=query_points, point_size=15.0, opacity=query_opacity / query_opacity.max())#, custom_data={'attention': query_attention.cpu()})\n",
    "        grasp_geometry.append(query_pl)\n",
    "    fig_grasp = get_plotly_fig(\"Grasp\")\n",
    "    fig_grasp = fig_grasp.add_traces(grasp_geometry)\n",
    "\n",
    "\n",
    "\n",
    "    placement_geometry = []\n",
    "    best_sample_pcd = PointCloud.merge(scene_pcd, grasp_pcd.transformed(pose)[0])\n",
    "    best_sample_pl = best_sample_pcd.plotly(point_size=1.0)\n",
    "    placement_geometry.append(best_sample_pl)\n",
    "    if sampled_poses is not None:\n",
    "        sample_pl = PointCloud.points_to_plotly(pcd=sampled_poses.points, point_size=7.0, colors=[0.2, 0.5, 0.8])\n",
    "        placement_geometry.append(sample_pl)\n",
    "    fig_sample = get_plotly_fig(\"Sampled Placement\")\n",
    "    fig_sample = fig_sample.add_traces(placement_geometry)\n",
    "\n",
    "\n",
    "\n",
    "    return fig_grasp, fig_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and warm-up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# device = 'cpu'\n",
    "unit_len = 0.01\n",
    "##### Initialize Pick Agent #####\n",
    "pick_agent_config_dir = \"config/agent_config/pick_agent.yaml\"\n",
    "pick_agent_param_dir = \"checkpoint/mug_10_demo/pick/model_iter_600.pt\"\n",
    "max_N_query_pick = 1\n",
    "langevin_dt_pick = 0.001\n",
    "\n",
    "pick_agent = PickAgent(config_dir=pick_agent_config_dir, \n",
    "                       device = device,\n",
    "                       max_N_query = max_N_query_pick, \n",
    "                       langevin_dt = langevin_dt_pick).requires_grad_(False)\n",
    "\n",
    "pick_agent.load(pick_agent_param_dir)\n",
    "pick_agent.warmup(warmup_iters=10, N_poses=100, N_points_scene=2000)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([Rescale(rescale_factor=1/unit_len),\n",
    "                         Downsample(voxel_size=1.7, coord_reduction=\"average\"),\n",
    "                         NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))])\n",
    "grasp_proc_fn = Compose([\n",
    "                         Rescale(rescale_factor=1/unit_len),\n",
    "                         Downsample(voxel_size=1.4, coord_reduction=\"average\"),\n",
    "                         NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))])\n",
    "recover_scale = Rescale(rescale_factor=unit_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 0\n",
    "scene_raw, grasp_raw = get_raw_pointcloud(dir='demo/test_demo', idx=file_idx, pick_or_place='pick')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference: Sample grasp poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "\n",
    "##### Sample Pick Poses #####\n",
    "T_seed = 100\n",
    "pick_policy = 'sorted'\n",
    "pick_mh_iter = 1000\n",
    "pick_langevin_iter = 300\n",
    "pick_dist_temp = 1.\n",
    "pick_policy_temp = 1.\n",
    "pick_optim_iter = 100\n",
    "pick_optim_lr = 0.005\n",
    "\n",
    "Ts, edf_outputs, logs = pick_agent.forward(scene=scene_proc, T_seed=T_seed, policy = pick_policy, mh_iter=pick_mh_iter, langevin_iter=pick_langevin_iter, \n",
    "                                            temperature=pick_dist_temp, policy_temperature=pick_policy_temp, optim_iter=pick_optim_iter, optim_lr=pick_optim_lr)\n",
    "\n",
    "pick_poses = recover_scale(SE3(Ts.cpu()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_grasp, fig_sample = visualize(scene_pcd=scene_raw, grasp_pcd=grasp_raw, pose=pick_poses[0], sampled_poses=pick_poses, query=(edf_outputs['query_points'] * unit_len, edf_outputs['query_attention']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_grasp.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edf_standalone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3eb84ee738d48f759dfda703333a24b6fbfd3240801b95380e08874bd7a129f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
